[
  {
    "id": "4fe90f596a",
    "source": "Test Grid Spec - Phase 1",
    "body": "Welcome to the Nosana Test Grid Phase 1! This document contains everything you need to know before you participate. \nGetting started\nFor the duration of the first Test Grid, you will be assigned to a market with similar devices. The instructions in your email contain details about your market access key and how to connect with it. If you have not received this email yet please be patient, it should arrive within the first week. You should not switch to a different market unless requested to do so by the Nosana team.\nRewards\nDuring the Test Grid, you will get paid in NOS for the computations you perform while running jobs as a Nosana Node. Additional rewards can be earned by completing certain challenges. The reward specification sheet with all the details can be found in the Test Grid Reward Sheet.\nAll rewards are paid out in NOS tokens. The rewards listed in euros are converted to NOS at the market rate specified after the Test Grid.\n::: note\nThe rewards are subject to change, and can be updated by Nosana to reflect market conditions, improve fairness, or for any other reason.\n:::\nCompute Job Rewards\nFor processing compute jobs on your GPU, you will be rewarded with a certain amount of NOS per second. The amount you earn depends on your GPU model. These rewards are paid out immediately upon completion and can be freely transferred.\nNote that job earnings are subsidized for the Test Grid and are higher than during normal operations. Also, there will be periods during Test Grid where your GPU is idle and does not earn rewards.\n::: note\n- To earn rewards, you must run a Nosana Node and let it successfully do compute jobs\n- You only earn rewards while processing compute jobs. During idle times, your GPU is not utilized and you will not earn rewards.\n- Compute Rewards are directly transferred to your node’s Solana account \n:::\nAdditional Rewards\nThere are multiple tasks that can be completed for additional rewards, like sharing your experience on social networks or publishing educational content. These rewards are paid out at the end of the Test Grid and are subject to several conditions: \n::: note\n- You must perform the KYC process through our portal.\n- You may not be a resident of one of the listed restricted countries.\n- The amount is linearly unlocked over a period of 31 days, starting at the end of the Test Grid.\n- You will get access to our vesting portal, where vested tokens can be claimed at any time.\n- You must have completed at least 1 compute jobs on the Test Grid.\n- The additional tasks are subject to approval by the Nosana team.\n:::\nIf the requirements are not met, we will not be able to pay out additional rewards.\nRefer to the Test Grid Reward Sheet for details on the payment of tasks.\nDetails on the Additional Reward Types\nOnce you complete an additional task, fill out the following form: https://forms.gle/W4jjiTBSf5523TvU7\n Task Type Description Requirements Referrals Refer a friend to sign up for the waitlist for our Test Grid. They must fill in your node's address on the Wait List form and successfully register. Video Tutorial Create educational content in video format (YouTube/shorts/reels) (e.g., How to become a Node, How to monetize your GPU) See Video Requirements below Written Guide Create written educational content (tweet thread/blog post) (e.g., How to become a Node, How to monetize your GPU) See Written Content Requirements below Social Media Review Share your experience on Twitter, LinkedIn, Instagram, or TikTok (short review, mention benefits, etc.) Opinion Piece Write an opinion piece (article/blog) about the current state of AI Inference and GPU market, and how Nosana can address challenges  See Written Content Requirements Test Grid Feedback Form  Provide feedback on the Test Grid feedback form Form will be shared near the end of Test Grid Phase 1 Video Requirements:\n\nLanguage: Video content language is at your discretion, as long as it is well-spoken and demonstrates a high level of proficiency.\nPlatform: Video content must be uploaded to YouTube for longer tutorials, TikTok, Instagram, or YouTube shorts for shorter videos.\nQuality: Videos must be in 720p quality or higher.\nSound Quality: Ensure you are using a microphone for high-quality audio in the video.\nTask Demonstration: The video must effectively demonstrate the successful completion of the task from start to finish.\nQuality Evaluation: The quality will be evaluated by the Nosana core team\nTimeline: All video submissions must be made before the end of the Test Grid\n\nWritten Content Requirements:\n\nLanguage: Written content language is at your discretion, as long as it is well-written and demonstrates a high level of proficiency.\nPlatform: Written content must be published as a tweet thread, blog post, or any other appropriate platform.\nQuality: Ensure the written content is clear, well-structured, and provides valuable information.\nTask Description: Clearly describe the task or topic covered in the content.\nRecommended Lengths:\nOpinion Piece: Under 700 or 800 words.\nWritten Guide: At least 1,000 words. Longer blog posts need to contain actionable information that will help their readers solve problems.\n\nQuality Evaluation: Written content quality will be evaluated by the Nosana core team."
  },
  {
    "id": "b1b440ace3",
    "source": "Background",
    "body": "In 2021, our ambitious team, Nosana, based in Amsterdam, embarked on an extraordinary journey to revolutionize computing. Our goal was to build a decentralized crowd computer tailored for CPU computations, with a specific focus on transforming the field of CI/CD. We believed this technology could enable faster and more efficient software development and deployment processes.\nWith diverse backgrounds in software engineering, computer science, and artificial intelligence, we pooled our expertise to develop the platform. Through extensive research into distributed systems, blockchain technology, and computer hardware, we created a cutting-edge solution from scratch, leveraging each team member's unique insights and skills.\nWe successfully received a grant from the Solana Foundation, launched a token, and raised pre-seed funding for our vision.\nThroughout the development process, we encountered technical challenges and scalability issues. However, our unwavering determination and collective expertise propelled us forward. We continuously refined the system, prioritizing security, efficiency, and user-friendliness.\nWe have multiple large-scale crypto projects on the CI/CD engine, running numerous workloads on a daily basis.\nHowever, as we progressed, we realized that the CI/CD industry alone might not provide the scale we sought. We needed to pivot and explore broader applications for our decentralized crowd computer.\nThen we ran our GPU pilot with AI inference workloads.\nOur hard work began to pay off. We successfully started to pivot, harnessing the collective GPU power of users worldwide, hand in hand with the growing demand for AI workloads.\nToday, we stand at the forefront of a new era in GPU computing. Our decentralized crowd computer has exceeded our initial expectations, especially with the AI revolution in full swing and the growing demand for GPU devices distributed all over the globe in the houses of gamers, miners, and MacBook owners.\nAnd this is just the beginning. Stay up-to-date with our progress and news by reading our blog."
  },
  {
    "id": "d94df4cc74",
    "source": "Introduction",
    "body": "Welcome to the Nosana Network! Let us give you an introduction to Nosana and her vision\nWhat is Nosana?\nNosana is changing the way the world accesses and utilizes computing power. They are providing a distributed GPU grid that allows anyone to rent compute power without the usual overhead, excessive fees, or lock-in. Nosana is addressing the shortage of GPUs in the market, making it easier for companies to get access to the necessary hardware. They are also taking advantage of underutilized hardware, such as gaming PCs, miners, and MacBooks, and offering competitive prices for building and running AI-solutions. This makes Nosana an ideal choice for those looking for a reliable and affordable computing solution.\nWhat Nosana Offers\nNosana is a platform that provides AI users with affordable GPUs and GPU owners with an income. GPU owners can rent their GPUs to AI users, who in turn can access powerful hardware and train and use their models faster. Nosana also provides users with a powerful suite of tools to help them get the most out of their GPU resources. With easy-to-use APIs and flexible pricing options, Nosana makes it easier than ever to access the power of AI.\nUltimate Vision\nWe aim to bring the disruptive power of blockchain technology to the field of AI.\nAs we move toward a future that is more decentralized, our ultimate vision is that projects will not rely on centralized mega-corporations for their daily operations. We believe that communities can perform essential computations in a decentralized manner by utilizing the Nosana Network. With Nosana, a robust community has the ability to keep any project running forever.\nDisclaimer\nThis project has been created with the best efforts of the creator, and the claims, content, designs, algorithms, projections, roadmaps, specifications, and performance metrics are all made with due diligence. However, it is the responsibility of the reader to confirm the accuracy and veracity of the project, and nothing in this project should be seen as an investment solicitation. The creator cannot guarantee the accuracy of the information and the reader should use their own discretion in assessing the accuracy of the project."
  },
  {
    "id": "7565571e7d",
    "source": "Roadmap",
    "body": "Welcome to the journey of Nosana, a platform that powers the AI revolution by creating the world's largest distributed GPU network. Our mission is to power the AI revolution with powerful computational resources. Our roadmap outlines the key stages of our growth. The journey is divided into five major release cycles, each named after a galaxy, symbolizing our quest to explore the vast universe of possibilities.\n\nGenesis: The birth of our platform, a stage of rigorous testing and refining.\nGalactica: The expansion phase, where we open up our network to the public.\nTriangulum: Our step towards increased interoperability, integrating with major machine learning protocols.\nWhirlpool: Where we broaden our horizons, supporting a diverse array of GPU types and devices\nSombrero: Our commitment to cater to businesses of all sizes, with the introduction of financial and collaborative features.\n\nAt every stage, we focus on different aspects of our platform, aiming to provide a robust, scalable, and user-friendly GPU network. We eagerly anticipate the journey ahead and invite you to join us as we push the boundaries of what's possible.\nRelease cycles\nGenesis (v0.1 - H2 2023)\nThis is the platform's launch, which includes alpha and beta stages. It will entail bootstrapping the GPU network and replicating demand > supply > demand throughput to test the system from start to finish. It will involve testing the system, onboarding early adopters, and gathering input for future upgrades. During this phase, the emphasis will be on guaranteeing the platform's stability, dependability, and performance.\nGalactica (v1.0 - H1/H2 2024)\nThe main grid will go live in the second phase. The CLI and SDK will be released, allowing anyone to connect to the network. In addition, a Container Node for consumer GPUs will be launched. The emphasis throughout this phase will be on scaling the network and engaging with a broader user base.\nTriangulum (v1.X - H2 2024)\nThe focus of this release is on integrating major machine learning protocols. The Community Connector Library, as well as official connectors for PyTorch, HuggingFace, TensorFlow, and others, will be released. During this phase, the platform's usability and compatibility with existing machine-learning tools and libraries will increase.\nWhirlpool (v1.X - H1 2025)\nThe platform's reach will be expanded even further, which will support a more diverse array of GPUs, including those from AMD, Intel, and Apple Silicon. The objective is to make Nosana the world's largest compute grid. Since it will be necessary to make sure the platform can handle a wide range of hardware and use cases, this phase will present major technical obstacles.\nSombrero (v1.X - H2 2025)\nSupport for medium-sized and large businesses will be added in this release, as well as fiat currency ramping, billing, and teams. Because the platform will need to attract and support a broader range of users and use cases, this phase will include not only technological development but also business and community growth.\nExpected Timeline\n```gantt\naxisFormat  %Y-%m\nsection Genesis\nTest Grid Phase 1 : done  , g1, 2023-12 , 120d\nTest Grid Phase 2 : active, g2, after g1, 120d\nTest Grid Phase 3 : g3, after g2, 90d\nsection Galactica\nGalactica - Main grid : m1, after g3,90d\nsection Triangulum\nTriangulum - Main protocols : t1, after m1,90d\nsection Whirlpool\nWhirlpool - Largest GPU grid : w1,after t1,90d\nsection Sombrero\nSombrero - All businesses : s1,after w1,90d\n```"
  },
  {
    "id": "217ad2cb8d",
    "source": "Cached Resources",
    "body": "Every Nosana Node will cache the resources they need to run a job. \nThese can either be Docker images or the inference models needed. \nThis is useful, because the main bottle neck to spinning up a Nosana Job is a downloading the assets to start the job.\nBy splitting apart the docker image and the inference model into separate parts, it becomes possible to make the docker image really small. \nThis helps us achieve a couple of things:\n\nReduce Docker pull speed bottlenecks\nUse one Docker image to run multiple models\nShare inference models across Docker images, reducing disk space requirements and increasing time to first token.\n\nSoon you will be able to use the resource property to pull inference models from IPFS and HuggingFace.\nAllowing you to pull, cache and auto-clean up for multiple resources.\nCache availability\nThe cache lifetime is currently set to 24 hours from the moment it is downloaded to a Nosana Node.\nThe downloaded Docker image and inference models will be available on the Node for 24 hours.\nNote this does not mean the cached asset is cached to all other nodes in that specific market.\nThat means if a job gets picked up by Nosana Node XXX in the 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf market, and a second job is posted in the same market and is picked up by Nosana Node YYY, those resources will need to be downloaded again.\nIt's only if Nosana Node XXX picks up the job again, will the cached resources be available.\nAvailable resources per market\nAll nodes will have certain Docker images and inference models always available, called required resources.\nThese are available on a per market basis.\nIt's possible to retrieve a list of these required resources such as Docker images on a Nosana Node, by going to the following URL:\nhttps://dashboard.k8s.prd.nos.ci/api/markets/<Market-Address>/required-resources\nBy replacing <Market-Address> with the address of a market, you will be able to retrieve the cached resources for that specific Nosana Market.\nAn example of the 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf market would be the following.\nhttps://dashboard.k8s.prd.nos.ci/api/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf/required-resources\nWhen we go to this url, we will see the following JSON. \njson\n{ \"required_images\": [ \"registry.hub.docker.com/nosana/frpc:0.1.0\", \"registry.hub.docker.com/nosana/stats:v1.0.4\", \"registry.hub.docker.com/nosana/llm_benchmark:1.0.0\", \"registry.hub.docker.com/nosana/remote-resource-helper:0.1.0\", \"docker.io/openmmlab/lmdeploy:v0.5.3-cu12\", \"docker.io/vllm/vllm-openai:v0.5.4\", \"docker.io/saladtechnologies/a1111:ipv6-v1.9.4\" ], \"required_remote_resources\": [ { \"type\": \"S3\", \"url\": \"s3://nos-ai-models-qllsn32u/stable-diffusion/dreamshaper/8\" } ]\n}\nHere we can see the list of cached Docker images, and cached models that is always available for this particular Nosana Market. \nAll Nosana Nodes operating in this market will have these resources available. \nThat means that as soon as the job is posted to the market and picked up a Nosana Node, it will be almost instantly available.\nOtherwise the resources need to be downloaded, and then can take a while."
  },
  {
    "id": "f65409ad33",
    "source": "Endpoints",
    "body": "Setting Up and Communicating with Endpoints on the Nosana Network\nWhen posting a job to the Nosana network, it is possible to specify the duration for which you want your compute job to be available. This means an instance will be accessible via an endpoint with which you can communicate.\nThis guide will walk you through setting up an Nginx server and interacting with its endpoint. Afterwards, we will set up a Llama instance and start communicating with it.\nProof of concept: Nginx\nNginx is a high-performance web server and reverse proxy server that is widely used for serving static content, load balancing, and handling HTTP and HTTPS traffic.\nIt'll be a good proof of concept to showcase how to use a Nosana endpoint.\n\nSetting Up an Nginx Server\n\nStep 1: Define the Job Schema\nCreate a file called nginx.json and copy the following JSON schema into it:\njson\n{ \"version\": \"0.1\", \"type\": \"container\", \"meta\": { \"trigger\": \"cli\" }, \"ops\": [ { \"type\": \"container/run\", \"id\": \"nginx\", \"args\": { \"cmd\": [], \"image\": \"nginx\", \"expose\": 80 } } ]\n}\nThis schema specifies the use of the Nginx image and exposes port 80.\nStep 2: Post the Job\nRun the following command to post the job to the Nosana network:\nsh:no-line-numbers\nnosana job post --file nginx.json --market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nOnce the job is running, you will see an output similar to this:\n``sh:no-line-numbers _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nReading keypair from /home/djmbritt/.nosana/nosana_key.json\nNetwork: mainnet\nWallet: 4WtG17Vn3SSoTAVvXxpopQTG3Qo9NUK28Zotv4rL1ccv\nSOL balance: 0.05066028 SOL\nNOS balance: 66.781499 NOS\nipfs uploaded:  https://nosana.mypinata.cloud/ipfs/QmTcNQ4dGq8veeg8v5cQyGoaJEPSYQnVTd1TvfckVFVzRu\nposting job to market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf for price 0.000115 NOS/s (total: 0.8280 NOS)\njob posted with tx 4UJ7Ad84PkaxDvx7VQWwNfNia7M7E4WJQeAomjEuA8xn5V4T9QWbQtusJgsQUV9Dj9o8bs1FL6hJhhAPUrYeLVpF!\nService will be exposed at https://FhkRunC6dAtPaEWGJwRK16Vctzv1KmHBhpSyUmMsYyMS.node.k8s.prd.nos.ci\nJob: https://explorer.nosana.io/jobs/B3MmwHz7sovudYwMxZFwjS2E6eMRaEEqNgWao5RYUkLi\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmTcNQ4dGq8veeg8v5cQyGoaJEPSYQnVTd1TvfckVFVzRu\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000115 NOS/s\nStatus: RUNNING\nrun nosana job get B3MmwHz7sovudYwMxZFwjS2E6eMRaEEqNgWao5RYUkLi --network mainnet to retrieve job and result\n```\n::: info\nTake note of the following line in the output:\nThe service will be exposed at\nhttps://FhkRunC6dAtPaEWGJwRK16Vctzv1KmHBhpSyUmMsYyMS.node.k8s.prd.nos.ci\n:::\nNavigate to this link to find your Nginx service.\n\nSuccess! Your Nginx instance is running on the Nosana Network.\nOllama Inference Endpoint\nNow we will delve into how to setup an inference endpoint, where we will run an Ollama service and communicate with it.\nStep 1: Define the Job Schema\nCreate a file named ollama.json and paste the following JSON schema into it:\njson\n{ \"version\": \"0.1\", \"type\": \"container\", \"meta\": { \"trigger\": \"cli\" }, \"ops\": [ { \"type\": \"container/run\", \"id\": \"ollama\", \"args\": { \"cmd\": [ \"-c 'curl -s https://raw.githubusercontent.com/KeesGeerligs/nosana/main/benchmarking/images/command.sh -o /tmp/command.sh && chmod +x /tmp/command.sh && /tmp/command.sh'\" ], \"image\": \"docker.io/nosana/ollama-7b:0.0.1\", \"gpu\": true, \"expose\": 11434 } } ]\n}\nThis schema uses a Docker Ollama image provided by Nosana and exposes port 11434. The cmd array initializes the Ollama service.\nStep 2: Post the Job\nRun the following command to post the job:\nsh:no-line-numbers\nnosana job post --file ollama.json --market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPlease note that it can take up to 15-20 minutes for your endpoint to be available. The Nosana team is working on reducing startup time to a few seconds.\nOnce the job is running, you will see an output similar to this:\n``sh:no-line-numbers _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nReading keypair from /home/djmbritt/.nosana/nosana_key.json\nNetwork: mainnet\nWallet: 4WtG17Vn3SSoTAVvXxpopQTG3Qo9NUK28Zotv4rL1ccv\nSOL balance: 0.05492696 SOL\nNOS balance: 67.60951 NOS\nipfs uploaded:  https://nosana.mypinata.cloud/ipfs/QmcbDBHGVhdyYbUQs9sLwNzPUx1NcCoCXRWS7dA5VqJteZ\nposting job to market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf for price 0.000115 NOS/s (total: 0.8280 NOS)\njob posted with tx 5q6FcG1pNP9JWpcXYud2zK6pvWcCSGNVoayPxTRHQkxQytpmJdVAqXa6E3xxunFS8iLXs6vBERhvfxFZQJcmDFyd!\nService will be exposed at https://B7hWYTyWHMLmYUH9sZFJL3TjgrHUAdDBi76xuEg9m9hX.node.k8s.prd.nos.ci\nJob: https://explorer.nosana.io/jobs/3Rraj6gcv9YRcm1Euk6vnkNrh385woLLXQzetCnsFkTz\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmcbDBHGVhdyYbUQs9sLwNzPUx1NcCoCXRWS7dA5VqJteZ\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000115 NOS/s\nStatus: RUNNING\nrun nosana job get 3Rraj6gcv9YRcm1Euk6vnkNrh385woLLXQzetCnsFkTz --network mainnet to retrieve job and result\n```\nYou can visit the service URL to check the status.\n\nInitially, you might see a \"Page not found\" message. After 15 minutes, the Ollama service should be up and running, and you can start making requests to the endpoint.\n::: info\nNote the endpoint service url:\nhttps://B7hWYTyWHMLmYUH9sZFJL3TjgrHUAdDBi76xuEg9m9hX.node.k8s.prd.nos.ci\n:::\nThe Ollama service will run a server that is available at /api/generate.\nAppend /api/generate to the URL to communicate with the Ollama service.\nClient Example\nTry these client examples to talk to your endpoint, remember to fill in the endpoint with your own endpoint.\n:::: tabs\n@tab cURL\nUse the following curl command to post a JSON body to the endpoint:\nsh:no-line-numbers\ncurl -X POST https://8enyfpbp5pba3pwyexvhpw1jhfurou9tundyhdexfuuk.node.k8s.prd.nos.ci/api/generate \\\n-H \"Content-Type: application/json\" \\\n-d \"{\\\"model\\\": \\\"gemma:7b\\\", \\\"stream\\\": false, \\\"prompt\\\": \\\"What is water made of?\\\"}\"\n@tab JavaScript\nAlternatively, you can use JavaScript:\n```js\nimport https from \"https\";\nconst data = JSON.stringify({ model: \"gemma:7b\", stream: false, prompt: \"What is water made of?\",\n});\nconst options = { hostname: \"8enyfpbp5pba3pwyexvhpw1jhfurou9tundyhdexfuuk.node.k8s.prd.nos.ci\", port: 443, path: \"/api/generate\", method: \"POST\", headers: { \"Content-Type\": \"application/json\", \"Content-Length\": data.length, },\n};\nconst req = https.request(options, (res) => { let responseData = \"\"; res.on(\"data\", (chunk) => { responseData += chunk; }); res.on(\"end\", () => { console.log(\"Response:\", responseData); });\n});\nreq.on(\"error\", (e) => { console.error(\"Error:\", e);\n});\nreq.write(data);\nreq.end();\n```\n::::\nResponse\nThe endpoint should return a response like this:\njson\n{ \"model\": \"gemma:7b\", \"created_at\": \"2024-07-19T13:03:07.217265645Z\", \"response\": \"Water is composed of two hydrogen atoms and one oxygen atom bonded together.\", \"done\": true, \"done_reason\": \"stop\", \"context\": [ 968, 2997, 235298, 559, 235298, 15508, 235313, 1645, 108, 1841, 603, 2003, 1644, 576, 181537, 615, 235298, 559, 235298, 15508, 235313, 108, 235322, 2997, 235298, 559, 235298, 15508, 235313, 2516, 108, 11586, 603, 18588, 576, 1378, 20303, 25204, 578, 974, 16175, 24235, 74346, 3584, 35606, 615, 235298, 559, 235298, 15508, 235313, 108 ], \"total_duration\": 47917239075, \"load_duration\": 47700638869, \"prompt_eval_count\": 32, \"prompt_eval_duration\": 60460000, \"eval_count\": 15, \"eval_duration\": 112647000\n}\nThe service will be available for two hours before it gets taken down. Ensure you have enough NOS balance to cover this period; otherwise, you will be notified immediately.\nNext up, we will go through the ins and outs of how to write a job, and the specifications you can customize for each job."
  },
  {
    "id": "6142d029f0",
    "source": "Job Schema",
    "body": "The Nosana Job schema allows us to create a job definition and specify the parameters needed for our job. In this case, we will be posting a job to prompt a TinyLlama instance to create a story about \"Tony the tiny hawk.\" There are other features, such as specifying the running environment and triggers, but for now, we will only focus on the ops.args object of our schema definition.\nDefine Job Schema\n\nCreate a JSON file:\nName the file story.json and add the following content to define a job schema for running an AI inference task using the TinyLlama Model.\n\njson\n{ \"version\": \"0.1\", \"type\": \"container\", \"meta\": { \"trigger\": \"cli\" }, \"ops\": [ { \"type\": \"container/run\", \"id\": \"tinyllama\", \"args\": { \"cmd\": [\"'Write me a story about Tony the tiny hawk'\"], \"image\": \"docker.io/jeisses/tinyllama:v4\", \"gpu\": true } } ]\n}\nPosting the Job\nWhen you send a job to the Nosana network, you're essentially posting a job to a market. There is a Nosana Node with a NVIDIA GPU in queue for a specific market, and it will pick up new job postings.\nLet's post our job to the Nosana network on the following NVIDIA 4090 market address: EzuHhkrhmV98HWzREsgLenKj2iHdJgrKmzfL8psP8Aso\nsh:no-line-numbers\nnosana job post --file story.json --market RXP7JK8MTY4uPJng4UjC9ZJdDDSG6wGr8pvVf3mwgXF --wait\nAlternatively it is also possible to post to a specific GPU market using it's slug, such as nvidia-4090.\nUse the nosana market list command to take a look at the available slugs or take a look at Markets.\nsh:no-line-numbers\nnosana job post --file story.json --market nvidia-4090  --wait\nNow you should be able to see the job running on our network in the Status field.\n``sh:no-line-numbers{21} Running _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nReading keypair from /home/djmbritt/.nosana/nosana_key.json\nNetwork: mainnet\nWallet: 4WtG17Vn3SSoTAVvXxpopQTG3Qo9NUK28Zotv4rL1ccv\nSOL balance: 0.04303304 SOL\nNOS balance: 5.520211 NOS\nipfs uploaded:  https://nosana.mypinata.cloud/ipfs/QmVfCSRc7LmVUJQbJKJuvj9k61wsrDFUyfbv9pzntFoC1G\nposting job to market CA5pMpqkYFKtme7K31pNB1s62X2SdhEv1nN9RdxKCpuQ for price 0.00009 NOS/s (total: 0.6480 NOS)\njob posted with tx 3czE7QrbTnVPSWyAcuHDvj6TCLPG6xYNwWNab6Hs8PkW3RpNozhw8r1tLgNzSnNJJZeVBi3jVwHTFWgHW7Q8HSmw!\nService will be exposed at https://GTrY9X8AEHGHyCDS2py1sm4xQNUMkB5Zjy58sVYCTJ9t.node.k8s.prd.nos.ci\nJob: https://explorer.nosana.io/jobs/3brMatsFV2uNaY9VDMKdC3jmpBvq1GQs1o9nvddQqKoQ\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmVfCSRc7LmVUJQbJKJuvj9k61wsrDFUyfbv9pzntFoC1G\nMarket: https://explorer.nosana.io/markets/CA5pMpqkYFKtme7K31pNB1s62X2SdhEv1nN9RdxKCpuQ\nPrice: 0.00009 NOS/s\nStatus: RUNNING\n⠙ Waiting for job to complete\n```\nAfter a few seconds, you can see that the Status has changed to COMPLETED with the response to the prompt.\n```sh:no-line-numbers Completed\n-- Redacted\nNode: https://explorer.nosana.io/nodes/7WcQdNdReaz2FMMRrthxrELjfGtXwbKZXuDZUoha2Eis\nStart Time: Thu Jul 18 2024 09:29:28 GMT-0400 (Atlantic Standard Time)\nDuration: 12 seconds\nTotal Costs: 0.00108 NOS\nStatus: COMPLETED\nResult: https://nosana.mypinata.cloud/ipfs/QmUpv6jQmMjiJKYaozcLZvUcujkiWg8L3RWkvmX42xdYL7\nLogs:\n```\nAs mentioned before, you can retrieve the job and its potential results using the nosana job get command.\n\nsh:no-line-numbers\nnosana job get INSERT_JOB_ID --network mainnet --wait"
  },
  {
    "id": "75fd39b237",
    "source": "GPU Markets",
    "body": "Nosana offers a variety of GPU markets tailored to different needs. Below is a comprehensive list of available NVIDIA GPU markets. Market Name: Type of NVIDIA GPU available.\nAddress of Market: Address to use with the --market flag in the nosana job post command.\n\nFor real-time updates on prices, job timeouts, queue lengths, and more, visit the Nosana explorer.\nCLI commands\nOr it is also possible to retrieve market information through the @nosana/cli\nList Markets\nsh:no-line-numbers\nnosana market list\nOutput\n``sh:no-line-numbers _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nMarkets\n┌─────────────────────────────┬─────────────────────┬────────────────────────────────────────────────┐\n│ name │ slug │ address │\n├─────────────────────────────┼─────────────────────┼────────────────────────────────────────────────┤\n│  Market Enterprise 8xA5000  │ nvidia-8x-a5000 │  3XGECQon74HQwPJuZjgCwqdQ5Nt3wktZ9fcavcDN9qB2  │\n│ Market 3060 │ nvidia-3060 │  7AtiXMSH6R1jjBxrcYjehCkkSF7zvYWte63gwEDBcGHq  │\n│ Market 3070 │ nvidia-3070 │  RXP7JK8MTY4uPJng4UjC9ZJdDDSG6wGr8pvVf3mwgXF │\n│ Market 3080 │ nvidia-3080 │  7RepDm4Xt9k6qV5oiSHvi8oBoty4Q2tfBGnCYjFLj6vA  │\n│ Market 3090 │ nvidia-3090 │  CA5pMpqkYFKtme7K31pNB1s62X2SdhEv1nN9RdxKCpuQ  │\n│ Market 4060 │ nvidia-4060 │  47LQHZwT7gfVoBDYnRYhsYv6vKk8a1oW3Y3SdHAp1gTr  │\n│ Market 4070 │ nvidia-4070 │  EzuHhkrhmV98HWzREsgLenKj2iHdJgrKmzfL8psP8Aso  │\n│ Market 4080 │ nvidia-4080 │  77wdaAuYVxBW5u2QiqddkAzoBZ5cuKxH9ZCbx5HfFUb2  │\n│ Market 4090 │ nvidia-4090 │  97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf  │\n│ Market A5000 │ nvidia-a5000 │  4uBye3vJ1FAYukDdrvqQ36MZZZxqW3o8utWu8fyomRuN  │\n│ Market 6000/A6000 │  nvidia-6000-a6000  │  EjryZ6XEthz3z7nnLfjXBYafyn7VyHgChfbfM47LfAao  │\n│ Market A40 │ nvidia-a40 │  BLqSzPzcXMX5gseNXE4Ma45f31Eo6tNFVYoRmPG7kxP2  │\n│ Market A100 │ nvidia-a100 │  GLJHzqRN9fKGBsvsFzmGnaQGknUtLN1dqaFR8n3YdM22  │\n│ Market H100 │ nvidia-h100 │  Crop49jpc7prcgAcS82WbWyGHwbN5GgDym3uFbxxCTZg  │\n│ Market Laptop │ nvidia-laptop │  3EWVbggirRpDY2npzPDA7k21yzwz5wgwGxVVv6zCnRpa  │\n│ Market 4000/A4000 │  nvidia-4000-a4000  │  7fnuvPYzfd961iRDPRgMSKLrUf1QjTGnn7viu3P12Zuc  │\n│ Market A100 40GB │  nvidia-a100-40gb │  F3aGGSMb73XHbJbDXVbcXo7iYM9fyevvAZGQfwgrnWtB  │\n└─────────────────────────────┴─────────────────────┴────────────────────────────────────────────────┘\n```"
  },
  {
    "id": "6bb660ff65",
    "source": "Getting Started",
    "body": "The Nosana Network is a platform for running AI inference workloads. Currently, the best way to run inference on the Nosana network is through our CLI. We'll start with a simple hello world program and afterwards build on these concepts to create a fully working endpoint.\nInstalling The Nosana CLI\nTo begin, you need to install the Nosana CLI globally using NPM or your favorite JavaScript runtime.\n:::: tabs\n@tab npm \nNode.JS\nsh:no-line-numbers\nnpm install -g @nosana/cli\n@tab Yarn\nYarn\nsh:no-line-numbers\nyarn install -g @nosana/cli\n@tab pnpm\npnpm\nsh:no-line-numbers\npnpm install -g @nosana/cli\n@tab Bun\nBun\nsh:no-line-numbers\nbun install -g @nosana/cli\n::::\nVerify the installation by running:\nnosana --version\nYou should see the Nosana CLI usage information.\nYour Nosana Wallet\nWhen you first run the Nosana CLI, a new keypair is generated for you in ~/.nosana/.nosana_key.json, creating a new wallet. In order to run Nosana jobs, you need to have some NOS and SOL in this wallet. You can purchase NOS on Kraken or swap in your Solana wallet. \nNext, run nosana address, and the terminal will print out your address (the public key). If you need access to your private keys, they can be found in the local folder on your machine. However, for security reasons, we strongly advise against replacing this file.\nOnce you have NOS/SOL, send some to the address logged from the nosana address command. Now you're ready to start running jobs on the Nosana network!\nFirst Nosana Job\nOnce your wallet is loaded with some SOL and NOS, you can post jobs to the Nosana Network.\nFirst, we need to determine which market to post our job in. Navigate to Nosana Explorer Markets to see a list of available markets. Each market has certain parameters, such as NOS per second, type of GPU used, available nodes, etc. \nFor this first test, we will use the cheapest option, which at the time of writing is Market 3060. Copy the market address, 7AtiXMSH6R1jjBxrcYjehCkkSF7zvYWte63gwEDBcGHq, and use it in the nosana job post command.\nThe following parameters will be added to the command:\n\necho hello world - the shell command to run\n--wait - waits for the job to finish and posts the result back immediately\n--market - specifies the market to use\n\nnosana job post echo hello world --wait --market 7AtiXMSH6R1jjBxrcYjehCkkSF7zvYWte63gwEDBcGHq\nYou should see something similar to the following output in your console:\nGet Job from Job ID\nRetrieving the results from the CLI is also possible. Use the job ID to retrieve the data, which can be found in the job URL posted above:\nhttps://explorer.nosana.io/jobs/FQTP2F5hNP2rNGUtQm4Annrx462PgxPcSA6ND6ToPTxH.\nRun the following command to get the result of the job:\nnosana job get FQTP2F5hNP2rNGUtQm4Annrx462PgxPcSA6ND6ToPTxH"
  },
  {
    "id": "ca22a45b4a",
    "source": "How To Write A Job",
    "body": "Overview\n\nNosana Jobs: Each job on the Nosana network is essentially an AI inference task that runs from a Docker container. The jobs are designed to utilize GPU resources efficiently.\nNosana Nodes: Jobs run on different Nosana nodes, which are represented as Solana addresses in the Nosana explorer. Each node contributes its GPU resources to execute the jobs.\nGPU Markets: Different types of GPUs available in the network are categorized into various markets. Each market supports specific job requirements based on the type of GPU.\n\nExecution Flow\nThese steps describe how submitting an inference job to the Nosana network.\n\nJob Submission: A user submits a job defined in a JSON format. This job specifies the tasks to be executed, the Docker images to be used, and whether GPU resources are required.\nNode Selection: The job is assigned to a Nosana node based on its requirements. The nodes are identified by their Solana addresses.\nJob Execution: The selected node pulls the necessary Docker image and executes the commands specified in the job. For example, running AI inference tasks using the provided Python scripts.\nResource Utilization: Nodes utilize their GPU resources to run the tasks. This decentralized approach ensures efficient use of available hardware.\nCompletion and Rewards: Once the job is completed, the node may earn $NOS tokens as a reward for contributing its GPU resources.\n\nExamples\nNosana is a fully permissionless network, which means that whatever model you want to run in our network you can!\nFor examples you can take a look at the Examples Catalog or visit: Nosana-CLI Examples on GitHub.\nSome examples of which models you can run and links can be found in the following table:\n Model Family  Specialty llama2 text-to-text llama3 text-to-text tinyllama text-to-text stable diffusion text-to-image whisper audio-to-text Jupyter Notebook  Notebook backed by GPU \nNosana Inference example\nYou can explore the different jobs happening on the Nosana Explorer.\nHere we can see an example of a Nosana inference job. \nNote there are 2 ops (short for operations) happening in this inference job, both are using an Ubuntu Docker Container.\n\nThe first one is using the nvidia-smi and other utilities to log hardware specifications of the Nosana node.\nThe second element in the ops array is using Whisper to transcribe an audio file.\n\nThe cmd array is where we can input shell commands, we can specify the Docker image used, environment variables can be set using the env map, the GPU can be enable by setting the gpu value to true.\nRead below for a full list of all of the properties you can set.\njson\n{ \"version\": \"0.1\", \"type\": \"container\", \"meta\": { \"trigger\": \"cli\" }, \"ops\": [ { \"type\": \"container/run\", \"id\": \"gpu-stats\", \"args\": { \"cmd\": [ \"sh -c \", \"nvidia-smi; cat /proc/cpuinfo  grep flags  head -1;\" ], \"image\": \"ubuntu\", \"env\": { \"DEBUG\": \"1\" }, \"gpu\": true } }, { \"type\": \"container/run\", \"id\": \"run-whisper\", \"args\": { \"cmd\": [ \"sh -c \", \"wget -q https://nosana.mypinata.cloud/ipfs/QmPKP7hjBd1Yyt6CmVpggNCgn9x5oXD1ok27HQvmPiFyew -O audio.mp3;\", \"python openai-whisper.py -p audio.mp3;\", \"wget -q https://nosana.mypinata.cloud/ipfs/QmUFXcvn3KZNvQmND9SCtDnzsU4NzL6awwYTiCkTkdFNTd -O audio.mp3;\", \"python openai-whisper.py -p audio.mp3;\", // additional commands omitted for brevity ], \"image\": \"docker.io/nosana/whisper:cuda-check\", \"gpu\": true } } ]\n}\nJob Schema Specification\nBelow we can see a list of the properties and a description for each property that can be set in a Nosana Jofor each property that can be set in a Nosana Job.\n\nversion: Specifies Nosana compiler version.\ntype: Indicates the type of job. Here, it's set to \"container\", meaning the job will run inside a Docker container.\nmeta (optional):\ntrigger: Defines how the job is triggered. In this example, it's set to \"cli\" (command-line interface).\n\nglobal (optional): This a is a property to help define the properties of all of the operations defined in ops.\n\nimage: The Docker image to be used for the container.\ngpu: A boolean indicating whether GPU resources are required.\nentrypoint: An entry point in a container is a script or executable that specifies the command to be run when the container starts\nenv: Key value map for environment variables in the container.\nwork_dir: The working directory that will be selected for running commands.\n\nops: An array of operations that the job will perform. Each operation includes:\n\ntype: Specifies the operation type. For instance, \"container/run\" indicates a containerized operation.\nid: A unique identifier for the operation. Examples include \"gpu-stats\" and \"run-whisper\".\nresults(optional): - regex: Regex for parsing the result. - logType: The type of log for the result.\nargs: Arguments for the operation, which include:\nimage: The Docker image to be used for the container.\ncmd: The command(s) to be executed in the container.\nvolumes (optional): An array of objects containing a name and dest (destination) properties.\nexpose (optional): A number representing an application port that needs to be exposed via the Nosana Service Endpoint.\ngpu (optional): A boolean indicating whether GPU resources are required.\nwork_dir (optional): The working directory that will be selected for running commands.\noutput (optional): Specify the output\nentrypoint (optional): An entry point in a container is a script or executable that specifies the command to be run when the container starts\nenv (optional): Key value map for environment variables in the container.\nresources (optional): An array containing assets needed for inference.\ntype: String representing where the asset should be retrieved from, S3, HF (HuggingFace) or IPFS.\nurl: URL representing where the asset is located.\ntarget: Location where the asset should be downloaded to.\n\ncmd\nThe cmd array is important to illustrate, because there are some nuances on how to use it.\nIf you are familiar with how to use the cmd property in Docker, you should already have an idea of how this property works.\nThere are two primary ways of using the cmd array.\n\nString based CMD\nArray based CMD\n\nString based CMD\nWhen the first element of the array is the whole command, such as:\n\"gunicorn -w 4 -k uvicorn.workers.UvicornWorker main:app\"\nBash will be used as the shell to interpret this command.\nArray based CMD\nAnother option is to put each command and every flag as it's own element in an array:\n[\"/bin/sh\", \"-c\", gunicorn\", \"-w\", \"4\", \"-k\", \"uvicorn.workers.UvicornWorker\", \"main:app\"]\nWith the array based notation, we are able to specify the shell we want to use.\nNote that most often you will need to append -c flag after /bin/sh\nYou can read more about how to use the cmd property by going to the Docker Documentation.\nJob JSON schema\nHere you will be able to see the full JSON schema specification for a Nosana Job.\njson\n{ \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"definitions\": { \"JobType\": { \"type\": \"string\", \"enum\": [\"container\"] }, \"OperationType\": { \"type\": \"string\", \"enum\": [\"container/run\", \"container/create-volume\"] }, \"OperationArgsMap\": { \"type\": \"object\", \"properties\": { \"container/run\": { \"type\": \"object\", \"properties\": { \"image\": { \"type\": \"string\" }, \"cmd\": { \"oneOf\": [ { \"type\": \"string\" }, { \"type\": \"array\", \"items\": { \"type\": \"string\" } } ] }, \"volumes\": { \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\" }, \"dest\": { \"type\": \"string\" } }, \"required\": [\"name\", \"dest\"] } }, \"expose\": { \"type\": \"number\" }, \"gpu\": { \"type\": \"boolean\" }, \"work_dir\": { \"type\": \"string\" }, \"output\": { \"type\": \"string\" }, \"entrypoint\": { \"oneOf\": [ { \"type\": \"string\" }, { \"type\": \"array\", \"items\": { \"type\": \"string\" } } ] }, \"env\": { \"type\": \"object\", \"additionalProperties\": { \"type\": \"string\" } }, \"resources\": { \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"type\": { \"type\": \"string\" }, \"url\": { \"type\": \"string\" } \"target\": { \"type\": \"string\" } }, \"required\": [\"type\", \"url\", \"target\"] } }, }, \"required\": [\"image\", \"cmd\"] }, \"container/create-volume\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\" } }, \"required\": [\"name\"] } } }, \"OperationResults\": { \"type\": \"object\" }, \"Operation\": { \"type\": \"object\", \"properties\": { \"type\": { \"$ref\": \"#/definitions/OperationType\" }, \"id\": { \"type\": \"string\" }, \"args\": { \"type\": \"object\" }, \"results\": { \"$ref\": \"#/definitions/OperationResults\" } }, \"required\": [\"type\", \"id\", \"args\"] } }, \"type\": \"object\", \"properties\": { \"version\": { \"type\": \"string\" }, \"type\": { \"$ref\": \"#/definitions/JobType\" }, \"meta\": { \"type\": \"object\", \"properties\": { \"trigger\": { \"type\": \"string\" } } }, \"global\": { \"type\": \"object\", \"properties\": { \"image\": { \"type\": \"string\" }, \"gpu\": { \"type\": \"boolean\" }, \"entrypoint\": { \"oneOf\": [ { \"type\": \"string\" }, { \"type\": \"array\", \"items\": { \"type\": \"string\" } } ] }, \"env\": { \"type\": \"object\", \"additionalProperties\": { \"type\": \"string\" } }, \"work_dir\": { \"type\": \"string\" } } }, \"ops\": { \"type\": \"array\", \"items\": { \"$ref\": \"#/definitions/Operation\" } } }, \"required\": [\"version\", \"type\", \"ops\"]\n}"
  },
  {
    "id": "e08756aae5",
    "source": "Hello World",
    "body": "The Nosana job specification allows you to define and run any shell command to run inside a container.\nIn this example we will run an Ubuntu container to use the echo command to print \"hello world\"\nJSON Job Schema\nHere below we can see how with the ops array.\nEach job has a type, id, and an args object containing:\n- cmd: An array of shell commands that need to be executed, in this case the command echo hello world\n- image: The container image used to run the shell commands, in this case an Ubuntu container.\n::: info\nCreate a file named hello_world.json, copy and paste the following into it:\n:::\njson\n{ \"version\": \"0.1\", \"type\": \"container\", \"meta\": { \"trigger\": \"cli\" }, \"ops\": [ { \"type\": \"container/run\", \"id\": \"hello-world\", \"args\": { \"cmd\": [\"echo hello world\"], \"image\": \"ubuntu\" } } ]\n}\nCommand\nRun the following command to post the job to the network, we'll use the --wait flag to wait for the results.\nsh:no-line-numbers\nnosana job post --file hello_world.json --market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf --wait\nOutput\nWhen the Nosana job is posted successfully you should see something similar to the following:\n``sh:no-line-numbers _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nReading keypair from /home/djmbritt/.nosana/nosana_key.json\nNetwork: mainnet\nWallet: 4WtG17Vn3SSoTAVvXxpopQTG3Qo9NUK28Zotv4rL1ccv\nSOL balance: 0.02789192 SOL\nNOS balance: 54.511837 NOS\nipfs uploaded:  https://nosana.mypinata.cloud/ipfs/QmVp8m3Uq1Cm6JJ3NsuTMSGLNnqXa1mC85uV7YxBREQ78p\nposting job to market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf for price 0.000115 NOS/s (total: 0.8280 NOS)\njob posted with tx o52uZn3beCQAXe4rcvcsyRshCjGrFsFSJvRRUZZLpXNWxNArN7WzVjfZYNn6Sn15akqpM4fc4c45s7vxd49SAzK!\nService will be exposed at https://5L4SLcpXFiR5Fw3UvGzbSsZfXn66UMjgieTmmJj3kdwM.node.k8s.prd.nos.ci\nJob: https://explorer.nosana.io/jobs/58QFVuyJo6xuuUwtjSW4JvkJogNmt8zhec9SXsWW3mLr\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmVp8m3Uq1Cm6JJ3NsuTMSGLNnqXa1mC85uV7YxBREQ78p\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000115 NOS/s\nStatus: RUNNING\nNode: https://explorer.nosana.io/nodes/9ZNv6iqU1GbHjitG1QLpBU92Tc8wnQo2bMNLSZcDwkXA\nStart Time: Tue Jul 30 2024 14:44:53 GMT-0400 (Atlantic Standard Time)\nDuration: 8 seconds\nTotal Costs: 0.00092 NOS\nStatus: COMPLETED\nResult: https://nosana.mypinata.cloud/ipfs/QmPvpBfknTZVoWA3tNLk1VBoNksqAaFNvgogDAhkccPchS\nLogs:\n\nExecuted step hello-world in 0.111s\nhello world\n\nExited with status success with code 0\n```\nAs we can see the output of the commands we specified in the job are logged to the standard output, and displayed back to us in the logs of the results."
  },
  {
    "id": "1f7f3366b0",
    "source": "Jupyter",
    "body": "Harness the power of Nosana Endpoint to seamlessly run Jupyter Notebooks and connect via a user-friendly web interface. \nWith access to GPU-backed nodes, you can conduct your experiments efficiently and cost-effectively, unlocking new possibilities for your research and data analysis.\nJSON Job Schema\n::: info\nCreate a file named jupyter.json, copy and paste the following into it:\n:::\njson\n{ \"version\": \"0.1\", \"type\": \"container\", \"meta\": { \"trigger\": \"cli\" }, \"ops\": [ { \"type\": \"container/run\", \"id\": \"jupyter-notebook\", \"args\": { \"cmd\": [ \"bash -c \", \"source /etc/bash.bashrc && jupyter notebook --notebook-dir=/tf --ip 0.0.0.0 --no-browser --allow-root --NotebookApp.token='' --NotebookApp.password=''\" ], \"expose\": 8888, \"image\": \"tensorflow/tensorflow:latest-gpu-jupyter\", \"gpu\": true } } ]\n}\nCommand\nRun the following command to post the job to the Nosana network.\nsh:no-line-numbers\nnosana job post --file jupyter.json --market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nOutput\n``sh:no-line-numbers{16} _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nReading keypair from /home/djmbritt/.nosana/nosana_key.json\nNetwork: mainnet\nWallet: 4WtG17Vn3SSoTAVvXxpopQTG3Qo9NUK28Zotv4rL1ccv\nSOL balance: 0.0463936 SOL\nNOS balance: 65.953488 NOS\nipfs uploaded:  https://nosana.mypinata.cloud/ipfs/QmVYzwTJscCrt79guw3LBncorxxCTiHvoaSSFTmfJZmDoW\nposting job to market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf for price 0.000115 NOS/s (total: 0.8280 NOS)\njob posted with tx 3yK9LqRvL3jqxsX7swBBLQZb1nFwmAws32JWXZ644kbNTeje7Jgegb2i6tJJbXJ4vKsX2J17g3rbgPBZfycsfSCG!\nService will be exposed at https://4HQhVcQPfiNBFUHjLjEZa2YXZrF7xPoqLt7upnojp3Y7.node.k8s.prd.nos.ci\nJob: https://explorer.nosana.io/jobs/CZwK1SjaGuA6EVeChS8ZeiDztByjMjWyiCn37swcJthk\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmVYzwTJscCrt79guw3LBncorxxCTiHvoaSSFTmfJZmDoW\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000115 NOS/s\nStatus: RUNNING\nrun nosana job get CZwK1SjaGuA6EVeChS8ZeiDztByjMjWyiCn37swcJthk --network mainnet to retrieve job and result\n```\nExample"
  },
  {
    "id": "cb9dd5286e",
    "source": "Multi Operation Job",
    "body": "The Nosana job specification allows you to define multiple chunks of operations to be defined in one job.\nEach chunk of operations can happen in it's own predetermined container, making for any number of customization that your needs require.\nExample: You could start off your Nosana job in an Alpine container, and then hand off the results to a container that has an LLM, then over to an Ubuntu container for post processing.\nJSON Job Schema\nHere below we can see how with the ops array, we can define multiple JSON objects each containing it's own job.\nEach job has a type, id, and an args object containing:\n- cmd: An array of shell commands that need to be executed\n- image: The container image used to run the shell commands\n- volumes: Docker volumes that need to be attached to this operation\n- work_dir: The working directory within which the shell commands will run.\n::: info\nCreate a file named multi_job.json, copy and paste the following into it:\n:::\njson\n{ \"version\": \"0.1\", \"type\": \"container\", \"meta\": { \"trigger\": \"cli\" }, \"global\": { \"work_dir\": \"/home/\", \"env\": { \"DEBUG\": \"1\" } }, \"ops\": [ { \"type\": \"container/create-volume\", \"id\": \"create-volume\", \"args\": { \"name\": \"random-id-volume\" } }, { \"type\": \"container/run\", \"id\": \"run-from-cli\", \"args\": { \"cmd\": [ \"/bin/bash -c \", \"echo Hello World > /nosana/outputs/test.txt;\", \"ls /nosana/outputs;\", \"pwd;\" ], \"image\": \"ubuntu\", \"volumes\": [ { \"name\": \"random-id-volume\", \"dest\": \"/nosana/outputs\" } ], \"work_dir\": \"/home/podman\" } }, { \"type\": \"container/run\", \"id\": \"run-from-cli-2\", \"args\": { \"cmd\": \"/bin/bash -c 'echo Hello World; ls; pwd;'\", \"image\": \"ubuntu\" } } ]\n}\nCommand\nRun the following command to post the job to the network.\nsh:no-line-numbers\nnosana job post --file multi_job.json --market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nOutput\nWhen the Nosana job is posted successfully you should see something similar to the following:\n``sh:no-line-numbers _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nReading keypair from /home/djmbritt/.nosana/nosana_key.json\nNetwork: mainnet\nWallet: 4WtG17Vn3SSoTAVvXxpopQTG3Qo9NUK28Zotv4rL1ccv\nSOL balance: 0.03551372 SOL\nNOS balance: 54.52498 NOS\nipfs uploaded:  https://nosana.mypinata.cloud/ipfs/QmRSLxBU9iemsmjmHMmmqkwEkqujuYSrtPeNYqoNRGmmxT\nposting job to market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf for price 0.000115 NOS/s (total: 0.8280 NOS)\njob posted with tx 4tc9Ys9bGQSUKugVsgXKTa2GqVQkjTk99JvsiRVpchxW9fupsTyCeUiHN3fXqq7y9g2CsJbSkx6gZjtDiPEY2gs2!\nService will be exposed at https://FiYK3EKgFRbD1yfcPMUMhxNEPz3bkpYduCmWt7hJKRtp.node.k8s.prd.nos.ci\nJob: https://explorer.nosana.io/jobs/3YBLQ1LAQyjsUyBBN3G8oNRc7fBSGU3qDhWc9VtMTtDA\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmRSLxBU9iemsmjmHMmmqkwEkqujuYSrtPeNYqoNRGmmxT\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000115 NOS/s\nStatus: RUNNING\nrun nosana job get 3YBLQ1LAQyjsUyBBN3G8oNRc7fBSGU3qDhWc9VtMTtDA --network mainnet to retrieve job and result\n```\nLet's run the nosana job get command to retrieve the results.\n``sh:no-line-numbers _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nNetwork: mainnet\nJob: https://explorer.nosana.io/jobs/3YBLQ1LAQyjsUyBBN3G8oNRc7fBSGU3qDhWc9VtMTtDA\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmRSLxBU9iemsmjmHMmmqkwEkqujuYSrtPeNYqoNRGmmxT\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000115 NOS/s\nStatus: COMPLETED\nNode: https://explorer.nosana.io/nodes/DKUo8PCKcrfunRiCHWFNjeVLQieVYWr8izauPCFm8BVv\nStart Time: Tue Jul 30 2024 09:30:50 GMT-0400 (Atlantic Standard Time)\nDuration: 41 seconds\nTotal Costs: 0.004715 NOS\nStatus: COMPLETED\nResult: https://nosana.mypinata.cloud/ipfs/QmPutzR15brP5kFKeySRRYRp1PUj2Dz91xhZS9dkP7yjGo\nLogs:\n\nExecuted step create-volume in 0.244s\n\nExited with status success with code 0\n\nExecuted step run-from-cli in 3.196s\ntest.txt\n/home/podman\n\nExited with status success with code 0\n\nExecuted step run-from-cli-2 in 0.306s\nHello World\nubuntu\n/home\n\nExited with status success with code 0\n```\nAs we can see the output of the commands we specified in the job are logged to the standard output, and displayed back to us in the logs of the results."
  },
  {
    "id": "2df1828550",
    "source": "Ollama",
    "body": "Get up and running with Llama 3.1, Mistral, Gemma 2, and other large language models.\nThis example will help you get setup with an Ollama instance running an inference model of your choice.\nWith which you can communicate via an API endpoint, learn more on how to use the API endpoint here: https://github.com/ollama/ollama/blob/main/docs/api.md\nJSON Job Schema\n::: info\nCreate a file named ollama.json, copy and paste the following into it:\n:::\njson\n{ \"version\": \"0.1\", \"type\": \"container\", \"meta\": { \"trigger\": \"cli\" }, \"ops\": [ { \"type\": \"container/run\", \"id\": \"ollama\", \"args\": { \"cmd\": [ \"-c 'curl -s https://raw.githubusercontent.com/KeesGeerligs/nosana/main/benchmarking/images/command.sh -o /tmp/command.sh && chmod +x /tmp/command.sh && /tmp/command.sh'\" ], \"image\": \"docker.io/nosana/ollama-7b:0.0.1\", \"gpu\": true, \"expose\": 11434 } } ]\n}\nCommand\nRun the following command to post the job to the Nosana network.\nsh:no-line-numbers\nnosana job post --file ollama.json --market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nOutput\nYou should be able to see something similar, here we need to take not of the Service URL.\n``sh:no-line-numbers{16} _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nReading keypair from /home/djmbritt/.nosana/nosana_key.json\nNetwork: mainnet\nWallet: 4WtG17Vn3SSoTAVvXxpopQTG3Qo9NUK28Zotv4rL1ccv\nSOL balance: 0.05492696 SOL\nNOS balance: 67.60951 NOS\nipfs uploaded:  https://nosana.mypinata.cloud/ipfs/QmcbDBHGVhdyYbUQs9sLwNzPUx1NcCoCXRWS7dA5VqJteZ\nposting job to market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf for price 0.000115 NOS/s (total: 0.8280 NOS)\njob posted with tx 5q6FcG1pNP9JWpcXYud2zK6pvWcCSGNVoayPxTRHQkxQytpmJdVAqXa6E3xxunFS8iLXs6vBERhvfxFZQJcmDFyd!\nService will be exposed at https://B7hWYTyWHMLmYUH9sZFJL3TjgrHUAdDBi76xuEg9m9hX.node.k8s.prd.nos.ci\nJob: https://explorer.nosana.io/jobs/3Rraj6gcv9YRcm1Euk6vnkNrh385woLLXQzetCnsFkTz\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmcbDBHGVhdyYbUQs9sLwNzPUx1NcCoCXRWS7dA5VqJteZ\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000115 NOS/s\nStatus: RUNNING\nrun nosana job get 3Rraj6gcv9YRcm1Euk6vnkNrh385woLLXQzetCnsFkTz --network mainnet to retrieve job and result\n```\nExample\nIt will take a while for this image to load up and be used, you might need to wait 15 minutes before the endpoint is up and running.\nThe end\n::: info\nNote the endpoint service url:\nhttps://B7hWYTyWHMLmYUH9sZFJL3TjgrHUAdDBi76xuEg9m9hX.node.k8s.prd.nos.ci\n:::\nClient Example\nTo communicate with the endpoint you will need to make HTTP requests to it.\nHere are a few examples to help you get started:\n:::: tabs\n@tab cURL\nUse the following curl command to post a JSON body to the endpoint:\nsh:no-line-numbers\ncurl -X POST https://B7hWYTyWHMLmYUH9sZFJL3TjgrHUAdDBi76xuEg9m9hX.node.k8s.prd.nos.ci/api/generate \\\n-H \"Content-Type: application/json\" \\\n-d \"{\\\"model\\\": \\\"gemma:7b\\\", \\\"stream\\\": false, \\\"prompt\\\": \\\"What is water made of?\\\"}\"\n@tab JavaScript\nAlternatively, you can use JavaScript:\n```js\nimport https from \"https\";\nconst data = JSON.stringify({ model: \"gemma:7b\", stream: false, prompt: \"What is water made of?\",\n});\nconst options = { hostname: \"https://B7hWYTyWHMLmYUH9sZFJL3TjgrHUAdDBi76xuEg9m9hX.node.k8s.prd.nos.ci\", port: 443, path: \"/api/generate\", method: \"POST\", headers: { \"Content-Type\": \"application/json\", \"Content-Length\": data.length, },\n};\nconst req = https.request(options, (res) => { let responseData = \"\"; res.on(\"data\", (chunk) => { responseData += chunk; }); res.on(\"end\", () => { console.log(\"Response:\", responseData); });\n});\nreq.on(\"error\", (e) => { console.error(\"Error:\", e);\n});\nreq.write(data);\nreq.end();\n```\n::::\nResponse\nYou should see the following response:\njson\n{ \"model\": \"gemma:7b\", \"created_at\": \"2024-07-29T14:04:54.272390422Z\", \"response\": \"Water is composed of two hydrogen atoms and one oxygen atom together. Its chemical formula is H2O.\", \"done\": true, \"done_reason\": \"stop\", \"context\": [ 968, 2997, 235298, 559, 235298, 15508, 235313, 1645, 108, 1841, 603, 2003, 1644, 576, 181537, 615, 235298, 559, 235298, 15508, 235313, 108, 235322, 2997, 235298, 559, 235298, 15508, 235313, 2516, 108, 11586, 603, 18588, 576, 1378, 20303, 25204, 578, 974, 16175, 24235, 74346, 3584, 235265, 9707, 9408, 10513, 603, 640, 235284, 235302, 35606, 615, 235298, 559, 235298, 15508, 235313, 108 ], \"total_duration\": 11622510413, \"load_duration\": 11219650532, \"prompt_eval_count\": 32, \"prompt_eval_duration\": 74712000, \"eval_count\": 23, \"eval_duration\": 279790000\n}"
  },
  {
    "id": "93bbc5a509",
    "source": "Open WebUI",
    "body": "Open WebUI is an extensible, feature-rich, and user-friendly self-hosted WebUI designed to operate entirely offline. \nIt supports various LLM runners, including Ollama and OpenAI-compatible APIs. \nWith Nosana we can run an instance of Open WebUI and connect to it via a Nosana Endpoint.\nJSON Job Schema\n::: info\nCreate a file named stable-webui.json, copy and paste the following into it:\n:::\njson\n{ \"version\": \"0.1\", \"type\": \"container\", \"meta\": { \"trigger\": \"cli\" }, \"ops\": [ { \"type\": \"container/run\", \"id\": \"open-webui\", \"args\": { \"cmd\": [], \"env\": { \"WEBUI_AUTH\": \"False\", \"WEBUI_NAME\": \"Nosana Chat\" }, \"image\": \"ghcr.io/open-webui/open-webui:ollama\", \"gpu\": true, \"expose\": 8080 } } ]\n}\nCommand\nRun the following command to post the job to the Nosana network.\nsh:no-line-numbers\nnosana job post --file open_webui.json --market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nOutput\n``sh:no-line-numbers _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nReading keypair from /home/djmbritt/.nosana/nosana_key.json\nNetwork: mainnet\nWallet: 4WtG17Vn3SSoTAVvXxpopQTG3Qo9NUK28Zotv4rL1ccv\nSOL balance: 0.04212692 SOL\nNOS balance: 65.125477 NOS\nipfs uploaded:  https://nosana.mypinata.cloud/ipfs/QmajmxiSFDfaZ4YmwsV92iqCz8LPZyLjKjjfqJ6m7kgWjw\nposting job to market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf for price 0.000115 NOS/s (total: 0.8280 NOS)\njob posted with tx 62TjXTM2rme78RCpM7pZ5YAkCHLmybqHvWrYfTigrjMooBqww99Qn1BoGWtkcWVA5CCuWy3H2PgANWr7PaqsWBtg!\nService will be exposed at https://ChiscJkoWFYwiZrCPzWd1wKBeDonMqaekq7eCAqTt1YC.node.k8s.prd.nos.ci\nJob: https://explorer.nosana.io/jobs/41s1bL8CYBctRgz6jCAYfUSxd8zueUkneHXHeMXVE3p5\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmajmxiSFDfaZ4YmwsV92iqCz8LPZyLjKjjfqJ6m7kgWjw\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000115 NOS/s\nStatus: RUNNING\nrun nosana job get 41s1bL8CYBctRgz6jCAYfUSxd8zueUkneHXHeMXVE3p5 --network mainnet to retrieve job and result\n```\nExample"
  },
  {
    "id": "8aab453205",
    "source": "Stable Diffusion WebUI",
    "body": "Stable Diffusion WebUI is a web interface for Stable Diffusion, implemented using Gradio library.\nUnleash your creativity with Nosana! Effortlessly run a Stable Diffusion instance to generate stunning images. \nExperience the power of advanced AI and GPU-backed nodes, making your image creation process smooth and efficient. \nWhether for personal projects or professional work, Nosana provides the tools you need to bring your artistic visions to life.\nJSON Job Schema\n::: info\nCreate a file named stable-webui.json, copy and paste the following into it:\n:::\njson\n{ \"version\": \"0.1\", \"type\": \"container\", \"meta\": { \"trigger\": \"cli\" }, \"ops\": [ { \"type\": \"container/run\", \"id\": \"stable-webui\", \"args\": { \"cmd\": [], \"image\": \"docker.io/universonic/stable-diffusion-webui:minimal\", \"gpu\": true, \"expose\": 8080 } } ]\n}\nCommand\nRun the following command to post the job to the Nosana network.\nsh:no-line-numbers\nnosana job post --file stable_webui.json --market Crop49jpc7prcgAcS82WbWyGHwbN5GgDym3uFbxxCTZg\nOutput\n``sh:no-line-numbers _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nReading keypair from /home/djmbritt/.nosana/nosana_key.json\nNetwork: mainnet\nWallet: 4WtG17Vn3SSoTAVvXxpopQTG3Qo9NUK28Zotv4rL1ccv\nSOL balance: 0.07779196 SOL\nNOS balance: 72.163245 NOS\nipfs uploaded:  https://nosana.mypinata.cloud/ipfs/QmbYeFzM6gNCf32GqQG2GDYQsFXxhBWYPskHHzooQSURBW\nposting job to market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf for price 0.000097 NOS/s (total: 0.6984 NOS)\njob posted with tx 3TveJgqzHiV1p97jxxzvS8Jeg29o7uKKtAJCU6NJ5SAtRsndtrkA6az3kLQwXp3aNatXD3ZUrBJ64YetWpTXsTFB!\nService will be exposed at https://GY1BDTVMQtwSJ7V3zFT5tYNimmyPRQfaJLKV1R5FDbz3.node.k8s.prd.nos.ci\nJob: https://explorer.nosana.io/jobs/5hYyrw4jBkekaLDZviAvoBJPXjfUgJu5S8u1fjzdt5Wx\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmbYeFzM6gNCf32GqQG2GDYQsFXxhBWYPskHHzooQSURBW\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000097 NOS/s\nStatus: RUNNING\nrun nosana job get 5hYyrw4jBkekaLDZviAvoBJPXjfUgJu5S8u1fjzdt5Wx --network mainnet to retrieve job and result\n```\nExample"
  },
  {
    "id": "59e7bcdd06",
    "source": "Tiny Llama",
    "body": "TinyLlama is a compact model with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.\nJSON Job Schema\n::: info\nCreate a file named tiny_llama.json, copy and paste the following into it:\n:::\njson\n{ \"version\": \"0.1\", \"type\": \"container\", \"meta\": { \"trigger\": \"cli\" }, \"ops\": [ { \"type\": \"container/run\", \"id\": \"tinyllama\", \"args\": { \"cmd\": [\"'Write me a story about Tony the tiny hawk'\"], \"image\": \"docker.io/jeisses/tinyllama:v4\", \"gpu\": true } } ]\n}\nCommand\nRun the following command to post the job to the Nosana network.\nsh:no-line-numbers\nnosana job post --file tiny_llama.json --market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nOutput\n``sh:no-line-numbers _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nReading keypair from /home/djmbritt/.nosana/nosana_key.json\nNetwork: mainnet\nWallet: 4WtG17Vn3SSoTAVvXxpopQTG3Qo9NUK28Zotv4rL1ccv\nSOL balance: 0.06854152 SOL\nNOS balance: 70.045108 NOS\nipfs uploaded:  https://nosana.mypinata.cloud/ipfs/QmZ9Zyt91LcDbbzHLSntAacr4UckYHy64UAxMtfpN3UWte\nposting job to market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf for price 0.000115 NOS/s (total: 0.8280 NOS)\njob posted with tx 66tKJ5HcRxFqKf98Z4t41qmHs3gyKCJGFwf1UkogiLT2TGgyudA9aFXhVNY2aVS71o4Safgt7UeocT4jRfMxRsDW!\nService will be exposed at https://BVDim8herHjb4yMPXRN2DdpxxbVw1nb9bu1oPhLFZgJc.node.k8s.prd.nos.ci\nJob: https://explorer.nosana.io/jobs/8qQeqpQBhZ1WitsKxQwSRyTNH1PF7cNAsFAmr66Nkrx1\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmZ9Zyt91LcDbbzHLSntAacr4UckYHy64UAxMtfpN3UWte\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000115 NOS/s\nStatus: RUNNING\nrun nosana job get 8qQeqpQBhZ1WitsKxQwSRyTNH1PF7cNAsFAmr66Nkrx1 --network mainnet to retrieve job and result\n```\nExample\nYou should get something similar to the following output, the output of the job will be printed after the job information logs:\n``sh:no-line-numbers\nnosana job get 8qQeqpQBhZ1WitsKxQwSRyTNH1PF7cNAsFAmr66Nkrx1 --network mainnet _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nNetwork: mainnet\nJob: https://explorer.nosana.io/jobs/8qQeqpQBhZ1WitsKxQwSRyTNH1PF7cNAsFAmr66Nkrx1\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmZ9Zyt91LcDbbzHLSntAacr4UckYHy64UAxMtfpN3UWte\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000115 NOS/s\nStatus: COMPLETED\nNode: https://explorer.nosana.io/nodes/HH57dEarbNVuFGyAmbs56HtKBekyaZAPVQ4KiLVRrZzy\nStart Time: Mon Jul 29 2024 08:22:40 GMT-0400 (Atlantic Standard Time)\nDuration: 17 seconds\nTotal Costs: 0.001955 NOS\nStatus: COMPLETED\nResult: https://nosana.mypinata.cloud/ipfs/QmbAAQfthnSvfAj1edVd6cxWAVko3S9J6nQaU1AdPQeaBK\nLogs:\n\nExecuted step ollama in 10.03s\nTony was a tiny hawk who lived in the ocean. One day, Tony decided to go for a swim, so he swam to a new place. When he arrived, he saw a small plane with a hook on the top. It was wet and quite ugly. Tony was very curious, so he peeked inside.\nHe saw it had many different buttons and looked really exciting. Tony smiled, excited to explore. He decided to fly around and explore. He began to sing and dance. Suddenly, Tony heard a loud ringing noise and the plane started to slide down the pole. Tony was scared, but then he saw a group of people jumping in the air like the plane.\nThe people started laughing, and Tony got a bit embarrassed. But he had a lot of fun and decided to explore another part of the airport. He kept singing and dancing, and eventually he made it safely to the right floor. The passengers cheered when they saw Tony fly in the air.\nTony was proud of himself for seeing something so amazing! He knew that from now on he was never going to go exploring again.achieved tok/s: 129.610116\n\nExited with status success with code 0\n```"
  },
  {
    "id": "a21e37c1e8",
    "source": "Whisper",
    "body": "Whisper is a general-purpose speech recognition model. \nIt is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.\nIn this example we will use whisper to analyze a file that has the audio of someone saying \"hello\", and it will transcribe the audio for us.\nJSON Job Schema\n::: info\nCreate a file named jupyter.json, copy and paste the following into it:\n:::\njson\n{ \"version\": \"0.1\", \"type\": \"container\", \"meta\": { \"trigger\": \"cli\" }, \"ops\": [ { \"type\": \"container/run\", \"id\": \"run-whisper\", \"args\": { \"cmd\": [\"python openai-whisper.py -p hello.mp3\"], \"image\": \"docker.io/nosana/whisper:latest\", \"gpu\": true } } ]\n}\nCommand\nRun the following command to post the job to the Nosana network.\nsh:no-line-numbers\nnosana job post --file whisper.json --market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nOutput\n``sh:no-line-numbers _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nReading keypair from /home/djmbritt/.nosana/nosana_key.json\nNetwork: mainnet\nWallet: 4WtG17Vn3SSoTAVvXxpopQTG3Qo9NUK28Zotv4rL1ccv\nSOL balance: 0.03786024 SOL\nNOS balance: 64.297466 NOS\nipfs uploaded:  https://nosana.mypinata.cloud/ipfs/QmSPZj2WxyxUJyMxqrn6iKUrHezKcnjNKdWbi4VLDSGQBB\nposting job to market 97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf for price 0.000115 NOS/s (total: 0.8280 NOS)\njob posted with tx 4j23fa4pNpggoexbP47Cw6oqRor2BJdH9PRAHNKbo396gttASv2KqZmfBDQN4MPs7L9oWeNJesfrekx2qP7Yf1ys!\nService will be exposed at https://7TsqMbiD6HPhJzHpMGB45fCPMwS5fZzCCgFHfhQZt5c7.node.k8s.prd.nos.ci\nJob: https://explorer.nosana.io/jobs/FKcGZH5xAfDycNi8F4KqYvPW6QA4Azu5AYsEDjT7UEus\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmSPZj2WxyxUJyMxqrn6iKUrHezKcnjNKdWbi4VLDSGQBB\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000115 NOS/s\nStatus: RUNNING\nrun nosana job get FKcGZH5xAfDycNi8F4KqYvPW6QA4Azu5AYsEDjT7UEus --network mainnet to retrieve job and result\n```\nExample\nAt the end of the output logs we will see the transcription.\n``sh:no-line-numbers _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nNetwork: mainnet\nJob: https://explorer.nosana.io/jobs/FKcGZH5xAfDycNi8F4KqYvPW6QA4Azu5AYsEDjT7UEus\nJSON flow:  https://nosana.mypinata.cloud/ipfs/QmSPZj2WxyxUJyMxqrn6iKUrHezKcnjNKdWbi4VLDSGQBB\nMarket: https://explorer.nosana.io/markets/97G9NnvBDQ2WpKu6fasoMsAKmfj63C9rhysJnkeWodAf\nPrice: 0.000115 NOS/s\nStatus: COMPLETED\nNode: https://explorer.nosana.io/nodes/EQbm8WokowUrZ1aV1wDLB8TutPoYSphVrnrP7ig4C1z8\nStart Time: Mon Jul 29 2024 10:37:29 GMT-0400 (Atlantic Standard Time)\nDuration: 127 seconds\nTotal Costs: 0.014605 NOS\nStatus: COMPLETED\nResult: https://nosana.mypinata.cloud/ipfs/QmWVT554VBZ5UAa8uDgHKrcHEqtqCccCoESMnAWKwYLac4\nLogs:\n\nExecuted step run-whisper in 118.415s\nUsing device: cuda:0\n[00:00.000 --> 00:00.500]  Hello!\n\nExited with status success with code 0\n```"
  },
  {
    "id": "5d8b86d614",
    "source": "Run your Node on Test Grid",
    "body": "GPU Test Grid\nThis guide is for nodes that already successfully registered their node for the GPU Test Grid and got selected for Phase 1 or Phase 2. Congratulations for making it this far! Now its time to start up your node and start earning $NOS! If you got selected, your node received a special access NFT that can be used to join a GPU market on Test Grid.\n::: info Backup your Solana Key\nTo find your Node's Solana key, navigate to ~/.nosana/nosana_key.json. It is essential to back up this file to ensure its safety.\nYou can print your private key to the terminal and then copy it and store it in your password manager for example.\nsh:no-line-numbers\nsudo cat ~/.nosana/nosana_key.json\n:::\nNosana Test Grid Script\nWith just a single command in your command line, you can easily run your Nosana Node on your machine. Simply run the following command:\nsh:no-line-numbers\nbash <(wget -qO- https://nosana.io/testgrid.sh)\nIf everything is successful, your Nosana Node is now running in a docker container.\nRegister for Test Grid\nIf you didn't register yet for testgrid, you can register for the next phases. These guides are instructing you on how to establish your Nosana Node and partake in the Test Grid. The choice of guide depends on your operating system:\n\nWindows (WSL2): This guide is suitable for you if you're running Windows. WSL2 stands for Windows Subsystem for Linux 2, which allows you to run Linux on your Windows machine. Following this guide, you'll be able to set up your Nosana Node within this environment.\n\nUbuntu: If you're running Ubuntu, a popular distribution of Linux, you should follow this guide.\n\nAdvanced (optional)\nLaunching the Nosana Node with Custom Parameters\nYou can manually launch the Nosana Node to modify certain parameters:\n* Use the --podman parameter to direct to your Podman service if it's running elsewhere.\n* Use --volume to map your solana key to /root/.nosana/nosana_key.json within the Docker container if you wish to use your own key.\nsh:no-line-numbers\ndocker run \\ --pull=always \\ --network host  \\ --interactive -t \\ --volume ~/.config/solana/id.json:/root/.nosana/nosana_key.json \\ nosana/nosana-cli nosana node start \\ --network mainnet \\ --rpc your-rpc-here \\ --podman http://$(ip addr show eth0  grep -oP '(?<=inet\\s)\\d+(\\.\\d+){3}'):8080\nFAQ\n::: details Do I need to keep my node running at all times?\nYou don't have to keep your node running at all times. However, the more your node is running, the more jobs it'll be able to pick up, which equals more $NOS rewards.\n:::\n::: details Where can I see the status of my node?\nYou can see the status of your node by having a look in the logs. To view the logs run: \nsh:no-line-numbers\ndocker logs -f nosana-node\n:::\n::: details Where can I see how much $NOS I’ve earned so far?\nYou can see how much you've earned by checking your $NOS balance. If you imported your private key in a wallet you can see the $NOS balance in the wallet. Else go to a Solscan or Solana Explorer and fill in your node address to see your token balances.\n:::\n::: details Why is my node queued?\nNot at all times will there be enough jobs for all the nodes in a market. In that case a queue will form. When there's a new job available the first node in the queue will automatically pick it up.\n:::\n::: details Which position in the queue is my node?\nTo see the market queue, go to the markets page on the Nosana Explorer. Choose the market you are assigned to, on the market page it'll show you the queue.\n:::"
  },
  {
    "id": "61691c025a",
    "source": "Ubuntu - GPU Nosana Node",
    "body": "Greetings! This is your comprehensive guide to setting up the Nosana Node on an Ubuntu system. Whether you are a seasoned developer or new to the Linux world, this easy-to-follow tutorial will assist you in getting your Nosana Node operational on your Ubuntu setup. Let's dive in and start the process.\n\nInstall Docker\nInstall NVIDIA drivers and container toolkit\nRun the Nosana Node and register for Test Grid\n\nMake sure you have Ubuntu version 20.04 or higher. You can check your Ubuntu version with:\nsh:no-line-numbers\nlsb_release -a\nDocker\nBefore proceeding with the installation and configuration of Docker, it is important to ensure that the appropriate privileges have been assigned. To do so, please refer to the following links for detailed instructions on properly installing and configuring Docker:\n\nInstall Docker Engine\nManage Docker as a non-root user\n\nBy following these steps, you will be able to run the Nosana Node without the need for root privileges.\nNVIDIA\nTo fully utilize the GPU on the grid, we will need to install both the NVIDIA drivers and NVIDIA's CUDA Toolkit.\nNVIDIA Driver Installation Guide\nFollow these steps to install the NVIDIA drivers on your system:\n\nVisit the official NVIDIA website or the link provided (https://www.linuxbabe.com/ubuntu/install-nvidia-driver-ubuntu) to download the correct driver.\nOnce the download is complete, run the installer and follow the instructions provided.\nAfter installation, check that the correct driver is installed by using the command nvidia-smi in the terminal.\nIf the command displays the correct driver information, the installation was successful. If not, try reinstalling the driver or seeking further assistance.\n\nGuide to Install NVIDIA Container Toolkit\nTo install the NVIDIA Container Toolkit (nvidia-ctk), run the following commands: \nsh:no-line-numbers\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey  sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list  \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g'  \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \\ && \\ sudo apt-get update\nThen we can install the NVIDIA Container Toolkit package:\nsh:no-line-numbers\nsudo apt-get install -y nvidia-container-toolkit\nConfiguring the NVIDIA Container Toolkit on Linux\nAs we aim to run Podman within Docker, adhere to the Docker configuration instructions detailed here: Configuring Docker.\nExecute the following commands in your terminal:\nsh:no-line-numbers\nsudo nvidia-ctk runtime configure --runtime=docker\nSubsequently, restart Docker with:\nsh:no-line-numbers\nsudo systemctl restart docker\nNosana Test Grid Script\nWith just a single command in your command line, you can easily set up a Nosana Node on your machine. Simply run the following command:\nsh:no-line-numbers\nbash <(wget -qO- https://nosana.io/register.sh)\nPlease note that this script has certain requirements and is specifically designed to run without the need for sudo privileges. It's crucial to exercise caution when running any script from the internet with sudo privileges. Even in this case, it's advisable to thoroughly review the script before executing it on your system. You can review the script here: https://nosana.io/register.sh\nThe script performs a series of tests to verify the successful completion of the previous steps outlined in the guide.\nYou will see your node's information displayed in the following format.\n`` _ _ \\ ___  ___  __ _ _ __ __ _ \\ / _ \\/ __/ _  ' \\ / ` \\ () _ \\ ( ( ____/___/_, _,_\nReading keypair from ~/.nosana/nosana_key.json\nNetwork: mainnet\nWallet: SOL balance: 0E-9 SOL\nNOS balance: 0 NOS\nProvider: podman\n```\nTest Grid Registration Instructions\nWhen running the script it'll ask for some information: email, Discord & Twitter/X handle (optional). After filling in the information and agreeing to the terms & conditions, a benchmark will start. In this benchmark we will check the hardware of your node. \nIf the benchmark succeeds, you should see the following output:\nBenchmark finished\n================================\nThank you for registering for Nosana Node. \nWe'll review your registration and you will get an email from us if you are selected.\nCongratulations! :tada: You have completed the registration. If you are selected for Test Grid you will receive an email with more information.\n::: warning\nTo find your Node's Solana key, navigate to ~/.nosana/nosana_key.json. It is essential to back up this file to ensure its safety.\n:::\nOptional: Run Podman in Docker\nIf you're running Ubuntu natively, you can use Docker to initiate your Podman instance. The register.sh script accomplishes this in the final step, making this a non-mandatory step:\nsh:no-line-numbers docker run -d \\ --pull=always \\ --gpus=all \\ --name podman \\ --device /dev/fuse \\ --privileged \\ -e ENABLE_GPU=true \\ -p 8080:8080 \\ nosana/podman podman system service --time 0 tcp:0.0.0.0:8080\nTo confirm GPU support within Podman containers, execute:\ndocker exec -it podman bash\npodman run --rm --device nvidia.com/gpu=all --security-opt=label=disable ubuntu nvidia-smi -L\nIf unsuccessful, ensure NVIDIA drivers and the nvidia-ctk are installed and configured\nIf you see Error: container create failed (no logs from conmon)... when running the command, follow the steps here to resolve issue\nTo validate Podman's proper functioning, use:\nsh:no-line-numbers\ncurl http://localhost:8080/v4.5.0/libpod/info\nLaunching the Nosana Node with Custom Parameters\nYou can manually launch the Nosana Node to modify certain parameters:\n* Use the --podman parameter to direct to your Podman service if it's running elsewhere.\n* Use --volume to map your solana key to /root/.nosana/nosana_key.json within the Docker container if you wish to use your own key.\nsh:no-line-numbers\ndocker run \\ --pull=always \\ --network host  \\ --interactive \\ --volume ~/.config/solana/id.json:/root/.nosana/nosana_key.json \\ nosana/nosana-node \\ --podman http://localhost:8080  \\ join-test-grid\nTroubleshoot\nIf you have questions or when you have error messages, please take a look at our Troubleshoot Guide or join our Discord for help."
  },
  {
    "id": "beee27c049",
    "source": "Windows - GPU Nosana Node",
    "body": "Welcome to the step-by-step guide on installing the Nosana Node on your Windows system. This documentation has been designed to make the installation process straightforward and efficient, even for those who aren't tech-savvy. Follow along, and you'll have your Nosana Node up and running in no time.\n\nInstall Ubuntu 22.04 on WSL2\nInstall Docker\nInstall NVIDIA drivers and container toolkit\nInstall Podman v4\nRun the Nosana Node and register for Test Grid\n\nGuide: Installing Ubuntu 22.04 on WSL2\nFor Windows users, it's essential to set up Ubuntu 22.04 specifically on WSL2.\n::: warning\nEnsure you're installing Ubuntu 22.04 on WSL2. Unfortunately, Ubuntu 20.04 is not compatible with WSL2.\n:::\nFor detailed instructions on how to install WSL and run Ubuntu 22.04, follow the tutorial linked below:\nHow to Install Ubuntu on WSL2\nOnce installed, you can verify your WSL2 Ubuntu version by running the following command:\nsh:no-line-numbers\nlsb_release -a\nPlease confirm that you have installed version 22.04.\nDocker\nTo ensure a successful setup, follow these steps to install and configure Docker:\n\nInstall Docker Desktop with the WSL2 backend by visiting this link: Install Docker Desktop with WSL2 backend.\n\nAfter installation, make sure Docker is added to its own user group.\n\nNVIDIA\nTo fully utilize the GPU on the grid, we will need to install both the NVIDIA drivers and NVIDIA's CUDA Toolkit.\nNVIDIA Driver Installation Guide\nTo use NVIDIA drivers, download and install the appropriate driver from the official NVIDIA website: NVIDIA Driver Downloads.\nTo verify if the drivers are correctly installed, open a terminal and run the following command:\n`sh:no-line-numbers\nnvidia-smi\nThese commands will help you generate the necessary configuration file and verify the CDI support.\nInstall the NVIDIA Container Toolkit\nTo install the NVIDIA Container Toolkit (nvidia-ctk), run the following commands:\nsh:no-line-numbers\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey  sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list  \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g'  \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \\ && \\ sudo apt-get update\nThen we can install the NVIDIA Container Toolkit package:\nsh:no-line-numbers\nsudo apt-get install -y nvidia-container-toolkit\nConfigure the NVIDIA Container Toolkit\nFor configuring the NVIDIA Container Toolkit to run Podman v4 natively on WSL2 (as Podman in Docker is not supported on WSL2), please follow the instructions for CDI configuration. You can find these instructions at https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/cdi-support.html.\nOnce you have completed the configuration, run the following commands:\nsh:no-line-numbers\nsudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\nsh:no-line-numbers\nnvidia-ctk cdi list\nPodman\nThe Nosana Node connects to Podman and runs your containers inside of it. On WSL2, you'll need to natively install Podman >v4.1:\nsh:no-line-numbers\necho \"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/unstable/xUbuntu_22.04/ /\"  sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:unstable.list\ncurl -fsSL https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/unstable/xUbuntu_22.04/Release.key  sudo gpg --dearmor  sudo tee /etc/apt/trusted.gpg.d/devel_kubic_libcontainers_unstable.gpg > /dev/null\nsudo apt update\nsudo apt install podman\nCheck if you have Podman version 4 installed and if you have GPU support:\npodman --version\npodman run --rm --device nvidia.com/gpu=all --security-opt=label=disable ubuntu nvidia-smi -L\nIf this doesn't work, make sure you have the NVIDIA drivers installed and the nvidia-ctk installed and configured\nIf you see Error: container create failed (no logs from conmon)... when running the command, follow the steps here to resolve issue\nNosana Test Grid Script\nWith just a single command in your command line, you can easily set up a Nosana Node on your machine. Simply run the following command:\nsh:no-line-numbers\nbash <(wget -qO- https://nosana.io/register.sh)\nTest Grid Registration Instructions\nWhen running the script it'll ask for some information: email, Discord & Twitter/X handle (optional). After filling in the information and agreeing to the terms & conditions, a benchmark will start. In this benchmark we will check the hardware of your node. \nIf the benchmark succeeds, you should see the following output:\nBenchmark finished\n================================\nThank you for registering for Nosana Node. \nWe'll review your registration and you will get an email from us if you are selected.\nCongratulations! :tada: You have completed the registration. If you are selected for Test Grid you will receive an email with more information.\n::: warning\nTo find your Node's Solana key, navigate to ~/.nosana/nosana_key.json. It is essential to back up this file to ensure its safety.\n:::\nAdvanced (optional)\nRun Podman API\nThis command can be used to start Podman service on port 8080, so our Nosana Node can reach it.\nThis is also already done by the register.sh script in the final step, so this step is optional:\nsh:no-line-numbers\npodman system service --time 0 tcp:0.0.0.0:8080&\nTo validate Podman's proper functioning, use:\nsh:no-line-numbers\ncurl http://localhost:8080/v4.5.0/libpod/info\nLaunching the Nosana Node with Custom Parameters\nYou can manually launch the Nosana Node to modify certain parameters:\n* Use the --podman parameter to direct to your Podman service if it's running elsewhere.\n* Use --volume to map your solana key to /root/.nosana/nosana_key.json within the Docker container if you wish to use your own key.\nsh:no-line-numbers\ndocker run \\ --pull=always \\ --network host  \\ --interactive \\ --volume ~/.config/solana/id.json:/root/.nosana/nosana_key.json \\ nosana/nosana-node \\ --podman http://$(ip addr show eth0  grep -oP '(?<=inet\\s)\\d+(\\.\\d+){3}'):8080 \\ join-test-grid\nTroubleshoot\nIf you have questions or when you have error messages, please take a look at our Troubleshoot Guide or join our Discord for help."
  },
  {
    "id": "4cc4d6e121",
    "source": "Getting Started",
    "body": "GPU Test Grid\nWelcome to the GPU Test Grid, a platform designed to validate our GPU grid across a diverse array of GPU devices. With comprehensive feedback and proactive issue resolution as our core objective, we are on the path to a smooth transition to the mainnet. We extend an open invitation to developers, data scientists, and AI enthusiasts to be part of this exciting journey. By joining the test grid and assisting us in running AI models, you'll also get the opportunity to participate in our reward distribution pool. For phase 1, we have allocated a substantial pool of 250,000 $NOS tokens. Together, let's shape the future of GPU testing and development.\nNosana Node\nThe Nosana Node is the software that you run to connect your hardware to the grid.\nIn order to register for the Test Grid, you'll need to run a Nosana Node with a NVIDIA GPU.\n::: warning\nAt the moment the Nosana Node is in pre-release. Therefore we highly recommend to run this in a clean environment or virtual machine, and to use a Solana address with only a minimal amount of SOL.\n:::\nHardware Requirements\n\nInternet connection\nRAM: 4GB+\nOne of the following NVIDIA GPUs: NVIDIA RTX 4090 NVIDIA RTX 4080 NVIDIA RTX 4070Ti  NVIDIA RTX 4070 NVIDIA RTX 4060Ti NVIDIA RTX 4060 NVIDA RTX 3090Ti  NVIDA RTX 3090 NVIDA RTX 3080Ti  NVIDA RTX 3080 NVIDIA RTX 3070Ti  NVIDA RTX 3070 NVIDA RTX 3060Ti NVIDA RTX 3060Ti  NVIDIA RTX A4000 NVIDIA RTX A4500 NVIDIA RTX A5000  NVIDIA RTX A5500 NVIDIA RTX A6000 Installation Guides\nThese guides are instructing you on how to establish your Nosana Node and partake in the Test Grid. The choice of guide depends on your operating system:\n\nWindows (WSL2): This guide is suitable for you if you're running Windows. WSL2 stands for Windows Subsystem for Linux 2, which allows you to run Linux on your Windows machine. Following this guide, you'll be able to set up your Nosana Node within this environment.\n\nUbuntu: If you're running Ubuntu, a popular distribution of Linux, you should follow this guide."
  },
  {
    "id": "e7b2ce1524",
    "source": "Troubleshooting Guide",
    "body": "This guide is created to aid users in resolving issues with their GPU-equipped Nosana Node configuration on both Linux and Windows operating systems.\nError Messages\nNvidia\n::: warning nvidia-smi: command not found\n::: details Solution\nIt means that you do not have NVIDIA drivers installed. To install them, download and install the correct drivers from the NVIDIA website: https://www.nvidia.com/download/index.aspx\n:::\n::: warning Error: setting up CDI devices: unresolvable CDI devices nvidia.com/gpu=all\n::: details Solution\nIt means that you did not install and configure the Nvidia Container Toolkit correctly:\n\nNvidia instruction for WSL2 (Windows)\nNvidia instructions for Ubuntu (Native) :::\n\nDocker\n::: warning The command 'docker' could not be found in this WSL 2 distro.\n::: details Solution\nEnsure that you have Docker Desktop installed and that it is running. If you still have this error message, check if Docker Desktop is using the WSL2 Backend (not Hyper-V). Follow this guide to turn on the WSL2 backend for Docker Desktop: https://docs.docker.com/desktop/wsl/.\nAlso check with Command Prompt or Powershell to make sure you have WSL version 2 installed:\nwsl -l -v\nAlso make sure your Ubuntu 22.04 distro is the default WSL distribution. The Docker-WSL integration is enabled on the default WSL distribution. To change your default WSL distro, run:\nwsl --set-default <distro name>\n:::\nPodman\n::: warning Error: Could not connect to Podman\n::: details Solution\nWhen you see this error go in Docker Desktop to -> Settings -> Docker engine, please add this line \"bip\":\"192.168.200.1/24\", somewhere after the first bracket, like this:\n{ \"bip\":\"192.168.200.1/24\",\n}\nThen click appy and restart Docker.\n:::\n::: warning Error: container create failed (no logs from conmon): conmon bytes \"\": readObjectStart: expect { or n, but found , error found in #0 byte of ......, bigger context ......\n::: details Solution\nThis error is caused by the latest version of conmon having known issues, downgrade conmon to resolve this, like this:\nwget https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_22.04/amd64/conmon_2.1.2~0_amd64.deb -O /tmp/conmon_2.1.2.deb\nsudo apt install /tmp/conmon_2.1.2.deb\nThen you can rerun the podmon command\npodman run --rm --device nvidia.com/gpu=all --security-opt=label=disable ubuntu nvidia-smi -L"
  }
]